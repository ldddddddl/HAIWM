name: xlstm_libero
state: training 
epochs: 200
lr: 1.0e-4
batchsize: 32
datasets_path: ./datasets/libero/libero_test
# datasets_path: /data1/donglingliu/libero/libero_test
results_path: ./results
# results_path: /data1/donglingliu/results
data_format: joints
is_init_param: True 
is_resume: False
checkpoint_path: ""
past_img_num: 5
future_img_num: 5
cropWidth: 112
cropHeight: 112
z_dim: 120
z_attention: true 
random_seed: 42
valid_datas_scale: 0.5
per_image_with_signal_num: 20 
cuda_visible_devices: 0
horizon: 50
action_dim: 7  # LIBERO: 6D末端执行器位姿 + 1D夹爪
num_workers: 0
max_history: 1
save_rate: 1.0
images_save_rate: 0.2
per_episode_to_batch_num: 2
enc_out_dim: 120
both_camera_concat_over: w_channel
is_use_cdna: True
model_save_interval: 50

# LIBERO specific settings
use_libero: True
task_suite: libero_10  # libero_10, libero_90, libero_spatial, libero_object, libero_goal

# Language modality settings
use_language: True
language_encoder_type: "clip"  # Options: "clip" or "onehot" (for ablation study)
clip_model: "ViT-B/32"
clip_embed_dim: 512
language_output_dim: 120  # Project CLIP embedding to match other modalities

# ablation
is_use_three_phase_train: False   
is_diff_generate_act: False
olny_action_generate: True  
is_only_transformer: False   
is_use_snn_residual: False 
is_use_prob_loss: True
is_use_mam: True           
visual_encoder: xlstm
act_encoder: transformerxlstm

# loss weights
alpha_loss:
    pos: 0.5
    vel: 0.3
    eff: 0.2 
    kl: 0.001
    actions: 1.0
    frames: 1.0
    sucker: 1.0  # End-effector gripper/sucker action loss weight
    gripper: 1.0  # Alias for sucker (LIBERO compatibility)
    gdl: 1.0

critic:
    num_heads: 4

transformer:
    num_layers: 6
    num_heads: 6
    embed_dim: 120

snn:
    is_use: False    
    T: 5
    is_use_spike_dataset: False
    is_sampling: False
    snn_dnc: 
        mlstm_block:
            mlstm:
                conv1d_kernel_size: 2
                qkv_proj_blocksize: 2
                num_heads: 3
        context_length: 16
        num_blocks: 4
        embedding_dim: 70
        slstm_at: []

model:
    mlstm_block:
        mlstm:
            conv1d_kernel_size: 4
            qkv_proj_blocksize: 4
            num_heads: 4
    slstm_block:
        slstm:
            backend: cuda
            num_heads: 4
            conv1d_kernel_size: 4
            bias_init: powerlaw_blockdependent
        feedforward:
            proj_factor: 1.3
            act_fn: gelu
    context_length: 256
    num_blocks: 2
    embedding_dim: 120
    dropout: 0.2
    slstm_at: [1]

act_model_dnc: 
    mlstm_block:
        mlstm:
            conv1d_kernel_size: 2
            qkv_proj_blocksize: 2
            num_heads: 4
    context_length: 120
    num_blocks: 2
    embedding_dim: 120
    slstm_at: []

act_model_enc: 
    mlstm_block:
        mlstm:
            conv1d_kernel_size: 2
            qkv_proj_blocksize: 2
            num_heads: 4
    context_length: 360
    num_blocks: 2
    embedding_dim: 120
    slstm_at: []

# WandB configuration
use_wandb: False
wandb_project: "HAIWM"
wandb_entity: null  # 可选，设置为您的wandb用户名或团队名
wandb_run_name: null  # 可选，默认使用时间戳

# Multi-GPU (DDP) training configuration
use_ddp: True  # 是否启用多卡训练
local_rank: -1  # 由启动脚本自动设置